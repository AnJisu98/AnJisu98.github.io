{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 학습용 샘플의 개수 : 20846\n",
      "전처리 후 테스트용 샘플의 개수 : 1629\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data=pd.read_csv('scaling.csv')\n",
    "test_data=pd.read_csv('test_file.csv')\n",
    "\n",
    "train_data['text'] = train_data['text'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train_data['text'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "train_data = train_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 학습용 샘플의 개수 :',len(train_data))\n",
    "\n",
    "\n",
    "test_data['text'] = test_data['text'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "test_data['text'].replace('', np.nan, inplace=True)\n",
    "test_data = test_data.dropna(how='any')\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pykospacing import spacing\n",
    "from soynlp.normalizer import *\n",
    "\n",
    "stopwords = ['뇨','하다','아','를','으로','은','는','의','을','과','함','해라'] # 불용어\n",
    "\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "for sentence in train_data['text']:\n",
    "    temp_X = []\n",
    "    sentence=repeat_normalize(sentence,num_repeats=2) # 'ㅋㅋㅋㅋ', '아하하하하' 같은 반복어구 2개로 제한\n",
    "    sentence=emoticon_normalize(sentence,num_repeats=2) # '옼ㅋㅋㅋㅋㅋ' -> '오ㅋㅋ'\n",
    "    kospacing_sent = spacing(sentence) # 띄어쓰기\n",
    "    temp_X = okt.morphs(kospacing_sent,stem=True) # 토큰화 \n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "    \n",
    "X_test = []\n",
    "for sentence in test_data['text']:\n",
    "    temp_X = [] \n",
    "    sentence=repeat_normalize(sentence,num_repeats=2)\n",
    "    sentence=emoticon_normalize(sentence,num_repeats=2)\n",
    "    kospacing_sent = spacing(sentence)\n",
    "    temp_X = okt.morphs(kospacing_sent,stem=True) \n",
    "    temp_X = [word for word in temp_X if not word in stopwords]\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "train_model = Word2Vec(sentences = X_train, size = 100, window = 5, min_count = 1, workers = 4, sg = 0)\n",
    "# 토큰화 완료된 단어 뭉치들을 바탕으로 워드투벡터 모델 이용\n",
    "# 양 옆으로 이웃한 4개의 단어들로 모델 구성\n",
    "# 데이터셋이 크지 않으므로 CBOW 방식 채택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17544, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17546\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "vocab_size=len(tokenizer.word_index)+2 # 0번 패딩 토큰, OOV 토큰을 고려해서 2개 추가\n",
    "print(vocab_size)\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train2 = tokenizer.texts_to_sequences(X_train)\n",
    "X_test2 = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=max(len(l) for l in X_train2)\n",
    "\n",
    "X_train2 = pad_sequences(X_train2, maxlen = max_len,padding='post')\n",
    "X_test2 = pad_sequences(X_test2, maxlen = max_len,padding='post')\n",
    "\n",
    "#같은 길이로 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.zeros((vocab_size,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_vector(word):\n",
    "    if word in train_model:\n",
    "        return train_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-7817a0b5b633>:2: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in train_model:\n",
      "<ipython-input-10-7817a0b5b633>:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  return train_model[word]\n"
     ]
    }
   ],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    temp=get_train_vector(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "e = Embedding(vocab_size, 100,trainable=True, weights=[embedding_matrix],input_length=max_len)\n",
    "model = Sequential()\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, input_dim=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6)\n",
    "mc = ModelCheckpoint('batch128_win5_100_notdupli_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "522/522 [==============================] - 6s 10ms/step - loss: 0.6074 - acc: 0.6713 - val_loss: 0.6528 - val_acc: 0.6892\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.68921, saving model to batch32_win5_300_notdupli_model.h5\n",
      "Epoch 2/30\n",
      "522/522 [==============================] - 5s 10ms/step - loss: 0.2979 - acc: 0.8895 - val_loss: 0.5873 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.68921 to 0.74077, saving model to batch32_win5_300_notdupli_model.h5\n",
      "Epoch 3/30\n",
      "522/522 [==============================] - 5s 10ms/step - loss: 0.1546 - acc: 0.9577 - val_loss: 0.6228 - val_acc: 0.7444\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74077 to 0.74436, saving model to batch32_win5_300_notdupli_model.h5\n",
      "Epoch 4/30\n",
      "522/522 [==============================] - 5s 10ms/step - loss: 0.0877 - acc: 0.9784 - val_loss: 0.5399 - val_acc: 0.8002\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.74436 to 0.80024, saving model to batch32_win5_300_notdupli_model.h5\n",
      "Epoch 5/30\n",
      "522/522 [==============================] - 6s 11ms/step - loss: 0.0469 - acc: 0.9922 - val_loss: 0.5175 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.80024 to 0.82014, saving model to batch32_win5_300_notdupli_model.h5\n",
      "Epoch 6/30\n",
      "522/522 [==============================] - 6s 11ms/step - loss: 0.0278 - acc: 0.9961 - val_loss: 0.5486 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82014\n",
      "Epoch 7/30\n",
      "522/522 [==============================] - 6s 11ms/step - loss: 0.0182 - acc: 0.9974 - val_loss: 0.6804 - val_acc: 0.7825\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82014\n",
      "Epoch 8/30\n",
      "522/522 [==============================] - 6s 11ms/step - loss: 0.0119 - acc: 0.9987 - val_loss: 0.6374 - val_acc: 0.8096\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.82014\n",
      "Epoch 9/30\n",
      "522/522 [==============================] - 6s 11ms/step - loss: 0.0075 - acc: 0.9993 - val_loss: 0.7150 - val_acc: 0.7957\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.82014\n",
      "Epoch 10/30\n",
      "522/522 [==============================] - 6s 11ms/step - loss: 0.0059 - acc: 0.9994 - val_loss: 0.7025 - val_acc: 0.8062\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82014\n",
      "Epoch 11/30\n",
      "522/522 [==============================] - 6s 11ms/step - loss: 0.0044 - acc: 0.9998 - val_loss: 0.7469 - val_acc: 0.8007\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.82014\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history=model.fit(X_train2, y_train, callbacks=[es, mc], batch_size=128, epochs=30, verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 2ms/step - loss: 0.6106 - acc: 0.7569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6106384992599487, 0.7569060921669006]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = load_model('batch32_win5_300_notdupli_model.h5')\n",
    "loaded_model.evaluate(X_test2, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted=loaded_model.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(y_test)):\n",
    "    if float(y_predicted[i])>=0.5:\n",
    "        y_predicted[i]=1\n",
    "    elif float(y_predicted[i])<0.5:\n",
    "        y_predicted[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    if y_predicted[i]==y_test[i]:\n",
    "        count=count+1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "print('accuracy: ', count / len(y_test))\n",
    "print(\"Precision, Recall and F1-Score:\\n\\n\", classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "  new_sentence = spacing(new_sentence)\n",
    "  new_sentence=emoticon_normalize(new_sentence,num_repeats=2)\n",
    "  new_sentence=repeat_normalize(new_sentence,num_repeats=2)\n",
    "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화 # 불용어 제거\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords]\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len,padding='post') # 패딩\n",
    "  score = float(loaded_model.predict(pad_new)) # 예측\n",
    "  if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 악플이 아닙니다.\\n\".format(score * 100 ))\n",
    "  else:\n",
    "    print(\"{:.2f}% 확률로 악플입니다.\\n\".format((1 - score)*100)) # 확인 파트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytchat import LiveChat\n",
    "import pafy\n",
    "import pandas as pd\n",
    "\n",
    "pafy.set_api_key(' ')\n",
    "\n",
    "video_id = ' '\n",
    "\n",
    "v = pafy.new(video_id)\n",
    "title = v.title\n",
    "author = v.author\n",
    "published = v.published\n",
    "\n",
    "print(title)\n",
    "print(author)\n",
    "print(published)\n",
    "empty_frame = pd.DataFrame(columns=['제목', '채널 명', '스트리밍 시작 시간', '댓글 작성자', '댓글 내용', '댓글 작성 시간'])\n",
    "empty_frame.to_csv('./youtube.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = LiveChat(video_id = video_id, topchat_only = 'FALSE')\n",
    "\n",
    "while chat.is_alive():\n",
    "    try:\n",
    "        data = chat.get()\n",
    "        items = data.items\n",
    "        for c in items:\n",
    "            new_sentence=spacing(c.message)\n",
    "            new_sentence=emoticon_normalize(new_sentence,num_repeats=2)\n",
    "            new_sentence=repeat_normalize(new_sentence,num_repeats=2)\n",
    "            new_sentence=okt.morphs(new_sentence,stem=True)\n",
    "            new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "            encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "            pad_new = pad_sequences(encoded, maxlen = max_len,padding='post') # 패딩\n",
    "            score = float(loaded_model.predict(pad_new)) # 예측\n",
    "            if(score > 0.5):\n",
    "                print(f\"{c.datetime} [{c.author.name}]- {c.message}\")\n",
    "            else:\n",
    "                data.tick()\n",
    "                data2 = {'제목' : [title], '채널 명' : [author], '스트리밍 시작 시간' : [published], '댓글 작성자' : [c.author.name], '댓글 내용' : [c.datetime], '댓글 작성 시간' : [c.message]}\n",
    "                result = pd.DataFrame(data2)\n",
    "    except KeyboardInterrupt:\n",
    "        chat.terminate()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
